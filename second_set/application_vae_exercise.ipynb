{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1224f385",
   "metadata": {},
   "source": [
    "# Building Autoencoders with PyTorch\n",
    "\n",
    "**Objective:** In this notebook, you will build and train two types of autoencoders—a standard autoencoder and a variational autoencoder (VAE)—to perform dimensionality reduction and generation on the MNIST dataset.\n",
    "\n",
    "**Content Credit:** This notebook is a guided exercise adapted from the excellent blog post [\"Intuitively Understanding Variational Autoencoders\"](https://avandekleut.github.io/vae/) by **Alexander Van de Kleut**. We highly recommend reading the original post for a deeper dive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting settings\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdad776",
   "metadata": {},
   "source": [
    "## Part 1: The \"Why\" - Motivation and Dimensionality Reduction\n",
    "\n",
    "Imagine we have a large, high-dimensional dataset, like the MNIST dataset of handwritten digits. Each image is 28x28 pixels, meaning each data point has 784 dimensions.\n",
    "\n",
    "The **manifold hypothesis** suggests that real-world high-dimensional data often lies on a much lower-dimensional \"manifold\" embedded within that high-dimensional space. This means that while the data has 784 dimensions, its underlying structure might be described by just a few.\n",
    "\n",
    "This is the motivation for **dimensionality reduction**: taking high-dimensional data and projecting it onto a lower-dimensional surface. This is a form of **unsupervised learning**, where the goal is not to predict a label, but to learn the underlying structure of the data itself.\n",
    "\n",
    "**Autoencoders** are a type of neural network designed specifically for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12726042",
   "metadata": {},
   "source": [
    "## Part 2: Standard Autoencoders\n",
    "\n",
    "An autoencoder is composed of two networks: an **encoder** and a **decoder**.\n",
    "\n",
    "1.  **Encoder ($e$):** This network learns a transformation $e: \\mathcal{X} \\rightarrow \\mathcal{Z}$ that projects the high-dimensional input data from space $\\mathcal{X}$ (e.g., 784 dimensions) to a lower-dimensional **latent space** $\\mathcal{Z}$ (e.g., 2 dimensions). The output, $z = e(x)$, is called a **latent vector**.\n",
    "\n",
    "2.  **Decoder ($d$):** This network learns a transformation $d: \\mathcal{Z} \\rightarrow \\mathcal{X}$ that projects the latent vector back into the original high-dimensional space, attempting to reconstruct the original input: $\\hat{x} = d(z) = d(e(x))$.\n",
    "\n",
    "The entire autoencoder is trained end-to-end by minimizing a **reconstruction loss**, which measures the difference between the original input $x$ and the reconstructed output $\\hat{x}$.\n",
    "\n",
    "![autoencoder](autoencoder-architecture.png)\n",
    "from https://lilianweng.github.io/posts/2018-08-12-vae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 1: Implement the Encoder ###\n",
    "\n",
    "# The Encoder takes a 784-dimensional image and maps it to a `latent_dims`-dimensional vector.\n",
    "# Your task is to define the two linear layers.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        # YOUR CODE HERE: Define a NN with input 784 to 512 neurons, then from 512 to the latent dimension. The activation for the first layer should be relu, while the second one should not be activated\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass defines how data flows through the layers.\n",
    "        # First, remmber to flatten the 28x28 image into a 784-dim vector.\n",
    "        # TO DO\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 2: Implement the Decoder ###\n",
    "\n",
    "# The Decoder takes a latent vector and attempts to reconstruct the original 784-dimensional image.\n",
    "# Your task is to define the two linear layers.\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        # YOUR CODE HERE: Define a NN from the latent dimension to 512 neurons, then from 512 to 784\n",
    "        # Activate the first layer with relu and the output with Sigmoid (to get pixel intensity between 0 and 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TO DO\n",
    "        # Finally, we reshape the 784-dim vector back to a 28x28 image.\n",
    "        return z.reshape((-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put everything together\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a6f09",
   "metadata": {},
   "source": [
    "## Part 3: Training and Evaluating the Autoencoder\n",
    "\n",
    "Now we'll write a function to train the autoencoder. The training process involves:\n",
    "1.  Passing a batch of images through the autoencoder to get reconstructions.\n",
    "2.  Calculating the reconstruction loss (we'll use Mean Squared Error).\n",
    "3.  Backpropagating the loss and updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 4: Implement the Training Loss ###\n",
    "\n",
    "def train(autoencoder, data, epochs=20):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data:\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Get the model's reconstruction\n",
    "            x_hat = ...\n",
    "            \n",
    "            # YOUR CODE HERE: Calculate the reconstruction loss.\n",
    "            # This should be the Mean Squared Error (MSE) between the\n",
    "            # original input `x` and the reconstruction `x_hat`.\n",
    "            # sum over the batch examples\n",
    "            loss = ...\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beef22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and Run Training ---\n",
    "\n",
    "# Set the latent dimension to 2 so we can visualize it.\n",
    "latent_dims = 2\n",
    "autoencoder = Autoencoder(latent_dims).to(device)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./data',\n",
    "               transform=torchvision.transforms.ToTensor(),\n",
    "               download=True),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder = train(autoencoder, data, epochs=5) # Using 5 epochs for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c2c0e",
   "metadata": {},
   "source": [
    "### Evaluating the Trained Autoencoder\n",
    "\n",
    "Once trained, we can do two interesting things:\n",
    "1.  **Visualize the Latent Space:** We can encode a batch of images and create a scatter plot of the resulting 2D latent vectors. We'll color-code the points by their digit label to see if the model has learned to cluster similar digits.\n",
    "2.  **Generate from the Latent Space:** We can sample points from the 2D latent space and pass them through the decoder. This allows us to see what kind of images the decoder generates from different regions of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3605674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide these helper functions for you.\n",
    "def plot_latent(autoencoder, data, num_batches=100):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        z = autoencoder.encoder(x.to(device))\n",
    "        z = z.to('cpu').detach().numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', s=10)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Latent Space of the Standard Autoencoder\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap='gray')\n",
    "    plt.title(\"Reconstructions from Latent Space\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent space\n",
    "plot_latent(autoencoder, data)\n",
    "\n",
    "# Plot reconstructions from the latent space\n",
    "plot_reconstructed(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24eb3e",
   "metadata": {},
   "source": [
    "### Analysis: The Problem with Standard Autoencoders\n",
    "\n",
    "Look at the plots above. The latent space shows clear clusters for different digits, which is great!\n",
    "\n",
    "However, look at the \"Reconstructions\" plot. You'll notice that there are \"gaps\" between the clusters in the latent space. If we sample a latent vector from one of these gaps (e.g., the top-left corner), the decoder produces a meaningless, blurry image. This is because the decoder was never trained on latent vectors from these regions.\n",
    "\n",
    "This makes standard autoencoders poor **generative models**. We can't just sample a random latent vector and expect a realistic output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3938c",
   "metadata": {},
   "source": [
    "## Part 4: Variational Autoencoders (VAEs)\n",
    "\n",
    "**Variational Autoencoders (VAEs)** solve this problem by making the latent space more continuous and structured.\n",
    "\n",
    "Instead of mapping an input `x` to a single latent point `z`, the VAE encoder maps it to a **probability distribution**—specifically, a Gaussian distribution defined by a mean `μ` and a standard deviation `σ`. A latent vector `z` is then *sampled* from this distribution.\n",
    "\n",
    "![vae](vae-gaussian.png)\n",
    "from https://lilianweng.github.io/posts/2018-08-12-vae/\n",
    "\n",
    "This has two effects:\n",
    "1.  **Robust Decoder:** The decoder learns to reconstruct images from a wider range of latent vectors, not just single points, making it more robust.\n",
    "2.  **Regularized Latent Space:** We add a new term to the loss function: the **Kullback-Leibler (KL) Divergence**. This term penalizes the learned distributions (`μ`, `σ`) for being too far from a standard normal distribution ($\\mathcal{N}(0, 1)$). This forces all the encoded distributions to cluster around the center of the latent space, filling in the gaps and creating a smooth, continuous space. It is given by \n",
    "$$\\mathbb{KL}\\left( \\mathcal{N}(\\mu, \\sigma) \\parallel \\mathcal{N}(0, 1) \\right) = \\sum_{x \\in X} \\left( \\sigma^2 + \\mu^2 - \\log \\sigma - \\frac{1}{2} \\right)$$\n",
    "\n",
    "The **total VAE loss** is: `Reconstruction Loss + KL Divergence Loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 5: Implement the Variational Encoder ###\n",
    "\n",
    "# The Decoder class remains the same. We only need to change the Encoder.\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "  \n",
    "        # YOUR CODE HERE: The VAE encoder needs one input layer of dim 784, relu activated, and two output layers, not activated: \n",
    "        # one for the mean (mu) and one for the log of the variance (log_sigma).\n",
    "        # Both should map from 512 to latent_dims.\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # for sampling on GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # remember to flatten the inputs\n",
    "        \n",
    "        \n",
    "        # Get mu and sigma from the final layers\n",
    "        mu =  ...\n",
    "        # use exp on log_sigma to ensure sigma is always positive\n",
    "        sigma = ...\n",
    "        \n",
    "        # YOUR CODE HERE: Sample the latent vector `z` from the distribution N(mu, sigma).\n",
    "        z = ...\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate the KL divergence.\n",
    "        self.kl = ...\n",
    "        \n",
    "        return z\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims) # The same decoder as before!\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 6: Implement the VAE Training Loss ###\n",
    "\n",
    "def train_vae(vae, data, epochs=20):\n",
    "    opt = torch.optim.Adam(vae.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data:\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # preidct using the model\n",
    "            x_hat = ...\n",
    "            \n",
    "            # YOUR CODE HERE: Calculate the total VAE loss.\n",
    "            # It's the reconstruction loss (like before) PLUS the KL divergence,\n",
    "            # which we stored as an attribute in our VariationalEncoder.\n",
    "            recon_loss = ...\n",
    "            kl_loss = ...\n",
    "            loss = recon_loss + kl_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, KL Div: {vae.encoder.kl.item():.4f}\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and Run VAE Training ---\n",
    "vae = VariationalAutoencoder(latent_dims).to(device)\n",
    "vae = train_vae(vae, data, epochs=5) # Using 5 epochs for speed\n",
    "\n",
    "# --- Plot the VAE results ---\n",
    "print(\"\\n--- VAE Results ---\")\n",
    "plot_latent(vae, data)\n",
    "plot_reconstructed(vae, r0=(-2, 2), r1=(-2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001ae89",
   "metadata": {},
   "source": [
    "### Analysis: The VAE Improvement\n",
    "\n",
    "Compare the VAE plots to the standard AE plots. What do you notice?\n",
    "\n",
    "1.  **The Latent Space is Compact:** The latent vectors are now tightly packed in a spherical cloud around the origin (0,0). The KL divergence forced this to happen.\n",
    "2.  **The Reconstructions are Smooth:** The space between digits is no longer a meaningless void. Instead, you see smooth transitions from one digit to another. The decoder has learned to generate plausible (if blurry) images from any point in this central region. The VAE is a true generative model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2541c9",
   "metadata": {},
   "source": [
    "## Part 5: Fun with Interpolation\n",
    "\n",
    "Because the VAE's latent space is smooth, we can do fun things like interpolate between two digits. We'll take a '1' and a '0', find their latent vectors `z1` and `z2`, and then decode points along the straight line connecting `z1` and `z2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 7: Implement Latent Space Interpolation ###\n",
    "\n",
    "def interpolate(autoencoder, x_1, x_2, n=12):\n",
    "    # Get the latent vectors for the two input images\n",
    "    z_1 = autoencoder.encoder(x_1)\n",
    "    z_2 = autoencoder.encoder(x_2)\n",
    "    \n",
    "    # YOUR CODE HERE: Create a list of `n` interpolated latent vectors.\n",
    "    # Start at z_1 and linearly move to z_2.\n",
    "    # Hint: z = z_1 + t * (z_2 - z_1), where t goes from 0 to 1.\n",
    "    z = torch.stack([z_1 + (z_2 - z_1)*t for t in np.linspace(0, 1, n)])\n",
    "    \n",
    "    # Decode the interpolated latent vectors\n",
    "    interpolate_list = autoencoder.decoder(z)\n",
    "    interpolate_list = interpolate_list.to('cpu').detach().numpy()\n",
    "\n",
    "    # Plot the results\n",
    "    w = 28\n",
    "    img = np.zeros((w, n*w))\n",
    "    for i, x_hat in enumerate(interpolate_list):\n",
    "        img[:, i*w:(i+1)*w] = x_hat.reshape(28, 28)\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# Grab a batch of data to find a '1' and a '0'\n",
    "x, y = next(iter(data))\n",
    "x_1 = x[y == 1][0].to(device).unsqueeze(0) # Find first '1'\n",
    "x_2 = x[y == 0][0].to(device).unsqueeze(0) # Find first '0'\n",
    "\n",
    "print(\"Interpolating with the VAE (smooth):\")\n",
    "interpolate(vae, x_1, x_2, n=20)\n",
    "\n",
    "print(\"\\nInterpolating with the standard AE (gappy):\")\n",
    "interpolate(autoencoder, x_1, x_2, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd548b64",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you have successfully built, trained, and evaluated both a standard autoencoder and a variational autoencoder.\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   **Standard Autoencoders** are effective for dimensionality reduction and learning compressed representations, but their latent spaces can be disjoint and irregular.\n",
    "*   **Variational Autoencoders** use a probabilistic encoder and a KL divergence loss to enforce a regular, continuous structure on the latent space.\n",
    "*   This regular structure makes **VAEs powerful generative models**, capable of creating new, realistic data by sampling from the latent space and decoding. The smooth transitions you saw during interpolation are a direct result of this improved structure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
